{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This web scraper can extract data from the given website list. Two layers of websites were explored. \n",
        "\n",
        "# Step 1\n",
        "## Put the mouse into the next cell, and click the left corner button of the below cell, which is the triangle button, to run the program. "
      ],
      "metadata": {
        "id": "zdlEuQkTtFPt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8mRrR3bm1X6"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "filename1 = next(iter(uploaded))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2\n",
        "## Run the next cell until a file generated and downloaded to your local compter. \n",
        "##  ScraperResult.csv \n",
        "## is the result\n"
      ],
      "metadata": {
        "id": "EZjxWZK8zC7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import logging\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "from googlesearch import search\n",
        "import requests\n",
        "import bs4\n",
        "import sys\n",
        "import re\n",
        "import requests\n",
        "import pymongo\n",
        "import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "import multiprocessing as mp\n",
        "\n",
        "df = pd.read_csv(io.BytesIO(uploaded[filename1]),usecols= ['Company Name','Website'])\n",
        "\n",
        "def get_phone():\n",
        "    try:\n",
        "        phone = soup.select(\"a[href*=callto]\")[0].text\n",
        "        return phone\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        phone = re.findall(r'\\(?\\b[2-9][0-9]{2}\\)?[-][2-9][0-9]{2}[-][0-9]{4}\\b', response.text)[0]\n",
        "        return phone\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        phone = re.findall(r'\\(?\\b[2-9][0-9]{2}\\)?[-. ]?[2-9][0-9]{2}[-. ]?[0-9]{4}\\b', response.text)[-1]\n",
        "        return phone\n",
        "    except:\n",
        "        #print ('Phone number not found')\n",
        "        phone = ''\n",
        "        return phone\n",
        "\n",
        "\n",
        "def get_email():\n",
        "    try:\n",
        "        email = re.findall(r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-]+\\.{1}[A-Za-z]{2,3})', response.text)[-1]\n",
        "        return email\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        email = soup.select(\"a[href*=mailto]\")[-1].text\n",
        "        return email\n",
        "    except:\n",
        "        #print ('Email not found')\n",
        "        email = ''\n",
        "        return email\n",
        "\n",
        "def get_address():\n",
        "    try:\n",
        "        str1 = soup.find(id='address').get_text(strip=True)\n",
        "        str1 = str1.replace('\\xa0', ' ')\n",
        "        return str1\n",
        "    except:\n",
        "        str1 = ''\n",
        "        return str1\n",
        "\n",
        "def get_tags(str_tag):\n",
        "    try:\n",
        "        str1 = soup.find(str_tag).get_text(strip=True)\n",
        "        str1 = str1.replace('\\xa0', ' ')\n",
        "        if str1=='403 Forbidden':\n",
        "          str1 = ''\n",
        "\n",
        "        return str1\n",
        "    except:\n",
        "        return ''\n",
        "\n",
        "def scrape_contact_link():\n",
        "    reg = re.compile(r'(?i)contact\\x20{1,}us')\n",
        "    url_contact =''\n",
        "    for link in soup.find_all('a', href=True, text=reg): \n",
        "    # if links then goto next page\n",
        "      url_contact = link['href']\n",
        "\n",
        "    return url_contact\n",
        "\n",
        "# Feb. 18\n",
        "def scrape_contact_link1():\n",
        "    reg = re.compile(r'(?i)contact\\x20{1,}')\n",
        "    url_contact =''\n",
        "    for link in soup.find_all('a', href=True, text=reg): \n",
        "        # if links then goto next page\n",
        "        url_contact = link['href']\n",
        "\n",
        "    return url_contact\n",
        "\n",
        "# Feb. 5, 2022\n",
        "def scrape_next_layers_link(query_str):\n",
        "    reg = re.compile(r'(?i)'+ query_str + '\\x20{1,}us')\n",
        "    url_contact =''\n",
        "    for link in soup.find_all('a', href=True, text=reg): \n",
        "    # if links then goto next page\n",
        "        url_contact = link['href']\n",
        "\n",
        "    return url_contact\n",
        "\n",
        "def scrap_contact_info_email(soup2):\n",
        "    emailList=[]\n",
        "    mailtos = soup.select('a[href^=mailto]')\n",
        "    for i in mailtos:\n",
        "        href=i['href']\n",
        "        try:\n",
        "            str1, str2 = href.split(':')\n",
        "        except ValueError:\n",
        "            break\n",
        "        emailList.append(str2)\n",
        "\n",
        "    return ' '.join(emailList)\n",
        "\n",
        "\n",
        "src_df = df\n",
        "\n",
        "for i, row in src_df.iterrows():\n",
        "    url = 'http://' + row[1]\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
        "    except:\n",
        "        print ('Unsucessful: ' + str(response))\n",
        "        continue\n",
        "\n",
        "    addr = get_address()\n",
        "    phone = get_phone()\n",
        "    contact_email = get_email()\n",
        "    title = get_tags('title')\n",
        "      \n",
        "    contact_email = scrap_contact_info_email(soup)\n",
        "    contact_facebook = ''\n",
        "    contact_twitter = ''\n",
        "    contact_instagram = ''\n",
        "    contact_linkedin = ''\n",
        "    for link in soup.find_all(href=re.compile(r\"facebook\", re.I)):\n",
        "        #'(?i)facebook(?i)')): \n",
        "        contact_facebook = link['href']\n",
        "\n",
        "    for link in soup.find_all(href=re.compile(r\"twitter\", re.I)): \n",
        "        contact_twitter = link['href']\n",
        "\n",
        "    for link in soup.find_all(href=re.compile(r\"instagram\", re.I)): \n",
        "        contact_instagram = link['href']\n",
        "\n",
        "    for link in soup.find_all(href=re.compile(r\"linkedin\", re.I)): \n",
        "        contact_linkedin = link['href']\n",
        "\n",
        "\n",
        "    #url2 = scrape_contact_link()\n",
        "    # scrape next layers lins \n",
        "    # contact us\n",
        "    query_str = 'contact'\n",
        "    url2 = scrape_next_layers_link(query_str)\n",
        "    if url2:\n",
        "        try:\n",
        "            response2 = requests.get(url2)\n",
        "            soup2 = bs4.BeautifulSoup(response2.text, 'html.parser')\n",
        "            # Contac Us\n",
        "            contact_email = scrap_contact_info_email(soup2)\n",
        "            for link in soup2.find_all(href=re.compile(r\"facebook\", re.I)):\n",
        "                contact_facebook = link['href']\n",
        "\n",
        "            for link in soup2.find_all(href=re.compile(r\"twitter\", re.I)): \n",
        "                contact_twitter = link['href']\n",
        "\n",
        "            for link in soup2.find_all(href=re.compile(r\"instagram\", re.I)):\n",
        "                contact_instagram = link['href']\n",
        "\n",
        "            for link in soup2.find_all(href=re.compile(r\"linkedin\", re.I)): \n",
        "                contact_linkedin = link['href']\n",
        "\n",
        "            # Mission\n",
        "        except:\n",
        "            print ('Unsucessful_URL2: ' + str(response2))\n",
        "        #continue\n",
        "    \n",
        "    # About us\n",
        "    query_str = 'about'\n",
        "    url2 = scrape_next_layers_link(query_str)\n",
        "    if url2:\n",
        "        try:\n",
        "            response2 = requests.get(url2)\n",
        "            soup2 = bs4.BeautifulSoup(response2.text, 'html.parser')\n",
        "            # About Us\n",
        "            contact_email = scrap_contact_info_email(soup2)\n",
        "            for link in soup2.find_all(href=re.compile(r\"facebook\", re.I)):\n",
        "                contact_facebook = link['href']\n",
        "\n",
        "            for link in soup2.find_all(href=re.compile(r\"twitter\", re.I)): \n",
        "                contact_twitter = link['href']\n",
        "\n",
        "            for link in soup2.find_all(href=re.compile(r\"instagram\", re.I)):\n",
        "                contact_instagram = link['href']\n",
        "\n",
        "            for link in soup2.find_all(href=re.compile(r\"linkedin\", re.I)): \n",
        "                contact_linkedin = link['href'] \n",
        "        except:\n",
        "            print ('Unsucessful_URL2: ' + str(response2))\n",
        "        \n",
        "    \n",
        "    src_df.loc[i,'Phone Number'] = phone\n",
        "    src_df.loc[i,'Email'] = contact_email\n",
        "    src_df.loc[i,'Title'] = title\n",
        "    src_df.loc[i,'Address'] = addr\n",
        "    src_df.loc[i,'Linkedin'] = contact_linkedin\n",
        "    src_df.loc[i,'Twitter'] = contact_twitter\n",
        "    src_df.loc[i,'Instagram'] = contact_instagram\n",
        "    src_df.loc[i,'Facebook'] = contact_facebook\n",
        "\n",
        "    \n",
        "src_df.to_csv('ScraperResult.csv', index=False)\n",
        "files.download(\"ScraperResult.csv\")"
      ],
      "metadata": {
        "id": "6ou7ne-1m9h7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}